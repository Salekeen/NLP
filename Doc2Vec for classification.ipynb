{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'm gonna try Doc2Vec; a document embeddding technique for feature representation in NLP and I will use it for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets do some imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load the data first\n",
    "df = pd.read_csv(\"./Data/Sentiment and Emotion in Text/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>worry</td>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I should be sleep, but im not! thinking about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>worry</td>\n",
       "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@charviray Charlene my love. I miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2     sadness                Funeral ceremony...gloomy friday...\n",
       "3  enthusiasm               wants to hang out with friends SOON!\n",
       "4     neutral  @dannycastillo We want to trade with someone w...\n",
       "5       worry  Re-pinging @ghostridah14: why didn't you go to...\n",
       "6     sadness  I should be sleep, but im not! thinking about ...\n",
       "7       worry               Hmmm. http://www.djhero.com/ is down\n",
       "8     sadness            @charviray Charlene my love. I miss you\n",
       "9     sadness         @kelcouch I'm sorry  at least it's Friday?"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worry         7433\n",
       "neutral       6340\n",
       "sadness       4828\n",
       "happiness     2986\n",
       "love          2068\n",
       "surprise      1613\n",
       "hate          1187\n",
       "fun           1088\n",
       "relief        1021\n",
       "empty          659\n",
       "enthusiasm     522\n",
       "boredom        157\n",
       "anger           98\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check how many categories are there\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is certainly a multi class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment lets just use top 3 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortlist = ['neutral','happiness','worry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16759, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset = df[df['sentiment'].isin(shortlist)]\n",
    "df_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Removing @mentions and urls\n",
    " - use NLTK TWEET tokenizer\n",
    " - usual normalizations step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nltk.org/api/nltk.tokenize.casual.html\n",
    "# check the docs for details\n",
    "\n",
    "# preserve_case. By default, it is set to True. If it is set to False, then the tokenizer\n",
    "# will downcase everything except for emoticons.\n",
    "\n",
    "# strip_handles. By default, it is set to False. It specifies whether to remove Twitter handles\n",
    "# of text used in the tokenize method.\n",
    "\n",
    "\n",
    "tweeter = TweetTokenizer(strip_handles=True, preserve_case=False)\n",
    "mystopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# content : one feature vector AKA one sample\n",
    "\n",
    "\n",
    "def preprocess_corpus(texts):\n",
    "    def remove_stops_digits(tokens):\n",
    "        return [token for token in tokens if token not in mystopwords and not token.isdigit()]\n",
    "    return [remove_stops_digits(tweeter.tokenize(content)) for content in texts]\n",
    "\n",
    "\n",
    "mydata = preprocess_corpus(df_subset['content'])\n",
    "mycats = df_subset['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do train test split now\n",
    "train_data,test_data,train_cats,test_cats = train_test_split(\n",
    "    mydata,mycats,random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First you need to tag the data and make it ready for training, lets do that\n",
    "\n",
    "train_doc2vec = [TaggedDocument((d), tags=[str(i)]) for i,d in enumerate(train_data)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['going', 'start', 'using', 'fast', 'access', 'm.twitter.com', 'school'], tags=['0']),\n",
       " TaggedDocument(words=['happy', 'star', 'wars', 'day', '!'], tags=['1']),\n",
       " TaggedDocument(words=['getting', 'ready', 'school', 'hopfully', 'today', 'good', 'day'], tags=['2']),\n",
       " TaggedDocument(words=['pavement', 'boiling', 'hot', ',', 'dogs', 'limping', '.', 'guess', \"summer's\", 'officially', '.'], tags=['3']),\n",
       " TaggedDocument(words=['anything', 'accepted', 'except', 'christianity', '.', 'google', 'discussion', 'thread', '\"', 'sexuality', 'religion', '\"', 'sled', 'second', 'life', '.'], tags=['4'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc2vec[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "# Lets train my doc2vec model\n",
    "model = Doc2Vec(\n",
    "    vector_size=50,\n",
    "    alpha=0.025,\n",
    "    min_count=5,\n",
    "    dm=1,\n",
    "    epochs=100\n",
    ")\n",
    "\n",
    "model.build_vocab(train_doc2vec)\n",
    "model.train(\n",
    "    train_doc2vec,\n",
    "    total_examples=model.corpus_count,\n",
    "    epochs=model.epochs\n",
    ")\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Size is around 4MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors =  [model.infer_vector(list_of_tokens) for list_of_tokens in train_data]\n",
    "test_vectors = [model.infer_vector(list_of_tokens) for list_of_tokens in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.32290205, -0.1476218 ,  0.19532248, -0.07156629,  0.12674208,\n",
       "       -0.48220414, -0.47260845,  0.05374419,  0.04003971,  0.47250116,\n",
       "       -0.04746957, -2.502907  ,  0.04083798, -0.43603304,  0.88077235,\n",
       "       -0.43018845, -1.1216704 ,  0.10489108, -0.4669931 ,  0.06127795,\n",
       "        0.5249197 , -0.3624527 ,  0.9820764 , -0.16685843,  0.65624183,\n",
       "        0.06044985, -0.8895854 ,  1.1260021 ,  0.28771064,  0.02224413,\n",
       "       -1.1069902 , -0.7974283 , -0.45196617,  0.02421224,  0.00730934,\n",
       "        0.37003765, -0.40049896,  0.26372626, -0.27938882, -0.18127759,\n",
       "        0.6289527 ,  0.48223737, -0.21647307,  1.0628011 ,  1.0584211 ,\n",
       "       -0.10752965, -0.02682967, -0.11258072, -0.4142734 , -0.0156419 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   happiness       0.35      0.52      0.42       724\n",
      "     neutral       0.45      0.56      0.50      1586\n",
      "       worry       0.60      0.37      0.46      1880\n",
      "\n",
      "    accuracy                           0.47      4190\n",
      "   macro avg       0.47      0.48      0.46      4190\n",
      "weighted avg       0.50      0.47      0.47      4190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "myclass = LogisticRegression(class_weight=\"balanced\")\n",
    "myclass.fit(train_vectors,train_cats)\n",
    "\n",
    "preds = myclass.predict(test_vectors)\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(test_cats,preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b308425c1363782e5433040c69737be01249237cf93c313549cb11eaac821734"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
